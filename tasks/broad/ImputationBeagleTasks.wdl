version 1.0

task CalculateContigsToProcess {
  input {
    File vcf_input
    Array[String] allowed_contigs
    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.5.0.0"
    Int cpu = 1
    Int memory_mb = 16000
    Int disk_size_gb = ceil(size(vcf_input, "GiB")) + 10
  }

  command <<<
    set -e -o pipefail

    # extract all chromosomes in input
    gunzip -c ~{vcf_input} | grep -v "#" | cut -f1 | sort -u > chromosomes.txt
    echo "Extracted chromosomes: $(cat chromosomes.txt | tr '\n' ' ')"

    # filter to allowed contigs, enforcing order
    for chr in ~{sep=" " allowed_contigs}; do
        if grep -q "^${chr}$" "chromosomes.txt"; then
            echo "${chr}" >> filtered_chromosomes.txt
        fi
    done

    echo "Filtered chromosomes: $(cat filtered_chromosomes.txt | tr '\n' ' ')"
  >>>

  output {
    Array[String] contigs_to_process = read_lines("filtered_chromosomes.txt")
  }
  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} SSD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }

}

task CountVariantsInChunks {
  input {
    File vcf
    File vcf_index
    File panel_bed_file

    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.5.0.0"
    Int cpu = 1
    Int memory_mb = 16000
    Int disk_size_gb = ceil((2 * size([vcf, vcf_index], "GiB")) + size(panel_bed_file, "GiB")) + 10
  }
  Int command_mem = memory_mb - 1500
  Int max_heap = memory_mb - 1000

  command <<<
    set -e -o pipefail

    ln -sf ~{vcf} input.vcf.gz
    ln -sf ~{vcf_index} input.vcf.gz.tbi

    gatk --java-options "-Xms~{command_mem}m -Xmx~{max_heap}m" CountVariants -V input.vcf.gz | tail -n 1 > var_in_original
    bedtools intersect -a ~{vcf} -b ~{panel_bed_file} | wc -l > var_also_in_reference
  >>>

  output {
    Int var_in_original = read_int("var_in_original")
    Int var_also_in_reference = read_int("var_also_in_reference")
  }
  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} HDD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
}

task CheckChunks {
  input {
    Int var_in_original
    Int var_also_in_reference

    String bcftools_docker = "us.gcr.io/broad-gotc-prod/imputation-bcf-vcf:1.0.7-1.10.2-0.1.16-1669908889"
    Int cpu = 1
    Int memory_mb = 4000
  }
  command <<<
    set -e -o pipefail

    if [ $(( ~{var_also_in_reference} * 2 - ~{var_in_original})) -gt 0 ] && [ ~{var_also_in_reference} -gt 3 ]; then
      echo true > valid_file.txt
    else
      echo false > valid_file.txt
    fi
  >>>
  output {
    Boolean valid = read_boolean("valid_file.txt")
  }
  runtime {
    docker: bcftools_docker
    disks: "local-disk 10 HDD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
}

task Phase {
  input {
    File dataset_vcf
    File ref_panel_bref3
    File genetic_map_file
    String basename
    String chrom
    Int start
    Int end

    String beagle_docker = "us.gcr.io/broad-gotc-prod/imputation-beagle:1.1.0-17Dec24.224-1758207261"
    Int cpu = 8                    # This parameter is used as the nthreads input to Beagle which is part of how we make it determinstic.  Changing this value may change the output generated by the tool
    Int memory_mb = 32000          # value depends on chunk size, the number of samples in ref and target panel
    Int disk_size_gb = ceil((3 * size(dataset_vcf, "GiB")) + size(ref_panel_bref3, "GiB")) + 10

    Array[Boolean] for_dependency         # used for task dependency management
  }

  command <<<
    set -e -o pipefail

    java -ea -XX:MaxRAMPercentage=90.0 -XX:MinRAMPercentage=90.0 -XX:-UseCompressedOops \
    -jar /usr/gitc/beagle.17Dec24.224.jar \
    gt=~{dataset_vcf} \
    ref=~{ref_panel_bref3} \
    map=~{genetic_map_file} \
    out=phased_~{basename} \
    chrom=~{chrom}:~{start}-~{end} \
    impute=false \
    nthreads=~{cpu} \
    seed=-99999

  >>>
  output {
    File vcf = "phased_~{basename}.vcf.gz"
    File log = "phased_~{basename}.log"
  }
  runtime {
    docker: beagle_docker
    disks: "local-disk ${disk_size_gb} SSD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
}

task Impute {
  input {
    File dataset_vcf
    File ref_panel_bref3
    File genetic_map_file
    String basename
    String chrom
    Int start
    Int end
    Boolean impute_with_allele_probabilities

    String beagle_docker = "us.gcr.io/broad-gotc-prod/imputation-beagle:1.1.0-17Dec24.224-1758207261"
    Int cpu = 8                    # This parameter is used as the nthreads input to Beagle which is part of how we make it determinstic.  Changing this value may change the output generated by the tool
    Int memory_mb = 32000          # value depends on chunk size, the number of samples in ref and target panel
    Int disk_size_gb = ceil((3 * size(dataset_vcf, "GiB")) + size(ref_panel_bref3, "GiB")) + 10
  }

  command <<<
    set -e -o pipefail

    java -ea -XX:MaxRAMPercentage=90.0 -XX:MinRAMPercentage=90.0 -XX:-UseCompressedOops \
    -jar /usr/gitc/beagle.17Dec24.224.jar \
    gt=~{dataset_vcf} \
    ref=~{ref_panel_bref3} \
    map=~{genetic_map_file} \
    out=imputed_~{basename} \
    chrom=~{chrom}:~{start}-~{end} \
    impute=true \
    ~{if impute_with_allele_probabilities then "ap=true" else ""} \
    nthreads=~{cpu} \
    seed=-99999

  >>>
  output {
    File vcf = "imputed_~{basename}.vcf.gz"
    File log = "imputed_~{basename}.log"
  }
  runtime {
    docker: beagle_docker
    disks: "local-disk ${disk_size_gb} SSD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
}

task ErrorWithMessageIfErrorCountNotZero {
  input {
    Int errorCount
    String message
  }
  command <<<
    if [[ ~{errorCount} -gt 0 ]]; then
      >&2 echo "Error: ~{message}"
      exit 1
    else
      exit 0
    fi
  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/ubuntu:20.04"
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
  output {
    Boolean done = true
  }
}

task CreateVcfIndex {
  input {
    File vcf_input

    Int disk_size_gb = ceil(1.1*size(vcf_input, "GiB")) + 10
    Int cpu = 1
    Int memory_mb = 6000
    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.5.0.0"
    Int preemptible = 3
  }
  Int command_mem = memory_mb - 1500
  Int max_heap = memory_mb - 1000

  String vcf_basename = basename(vcf_input)

  command {
    set -e -o pipefail

    ln -sf ~{vcf_input} ~{vcf_basename}

    bcftools index -t ~{vcf_basename}
  }
  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} SSD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }
  output {
    File output_vcf = "~{vcf_basename}"
    File output_vcf_index = "~{vcf_basename}.tbi"
  }
}

task LocalizeAndSubsetVcfToRegion {
  input {
    File vcf
    String output_basename
    String contig
    Int start
    Int end
    Boolean exclude_filtered = false

    Int disk_size_gb = ceil(3*size(vcf, "GiB")) + 10
    Int cpu = 1
    Int memory_mb = 6000
    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.6.1.0"
  }
  Int command_mem = memory_mb - 1500
  Int max_heap = memory_mb - 1000

  command {
    set -e -o pipefail

    # Recompress the VCF to ensure there isn't an issue with the input compression which we saw with java 17
    # around empty blocks being inserted in the compressed file which causes GATK to truncate the file
    # silently - https://broadworkbench.atlassian.net/browse/TSPS-612 for more info
    gunzip -c ~{vcf} | bgzip > recompressed.vcf.gz
    tabix recompressed.vcf.gz

    gatk --java-options "-Xms~{command_mem}m -Xmx~{max_heap}m" \
    SelectVariants \
    -V recompressed.vcf.gz \
    -L ~{contig}:~{start}-~{end} \
    -select 'POS >= ~{start}' ~{if exclude_filtered then "--exclude-filtered" else ""} \
    -O ~{output_basename}.vcf.gz
  }

  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} HDD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_vcf = "~{output_basename}.vcf.gz"
    File output_vcf_index = "~{output_basename}.vcf.gz.tbi"
  }
}

task SelectSamplesWithCut {
  input {
    File vcf

    Int cut_start_field
    Int cut_end_field
    String basename

    Int disk_size_gb = ceil(1.5 * size(vcf, "GiB")) + 10
    String bcftools_docker = "us.gcr.io/broad-gotc-prod/imputation-bcf-vcf:1.0.7-1.10.2-0.1.16-1669908889"
    Int cpu = 2
    Int memory_mb = 6000
  }

  command <<<
    set -euo pipefail

    mkfifo fifo_bgzip
    mkfifo fifo_cut

    bcftools view -h --no-version ~{vcf} | awk '!/^#CHROM/' > header.vcf
    n_lines=$(wc -l header.vcf | cut -d' ' -f1)

    cat header.vcf
    echo $n_lines

    bgzip -d ~{vcf} -o fifo_bgzip &
    tail +$((n_lines)) fifo_bgzip | cut -f 1-9,~{cut_start_field}-~{cut_end_field} > fifo_cut &

    cat header.vcf fifo_cut | bgzip -o ~{basename}.vcf.gz
  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/bcftools_bgzip:beagle_imputation_v1.0.0"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: memory_mb + " MiB"
    preemptible: 3
    maxRetries: 1
    noAddress: true
    cpu: cpu
  }

  output {
    File output_vcf = "~{basename}.vcf.gz"
  }
}

task MergeSampleChunksVcfsWithPaste {
  input {
    Array[File] input_vcfs
    String output_vcf_basename

    Int disk_size_gb = ceil(2.2 * size(input_vcfs, "GiB") + 50)
    Int mem_gb = 12
    Int cpu = 4
    Int preemptible = 3
  }

  command <<<
    set -euo pipefail

    vcfs=(~{sep=" " input_vcfs})

    mkfifo fifo_0
    mkfifo fifo_to_paste_0

    i=1

    fifos_to_paste=()
    md5sums=()
    bcftools view -h --no-version ${vcfs[0]} | awk '!/^#CHROM/' > header.vcf
    n_lines=$(wc -l header.vcf | cut -d' ' -f1)

    bgzip -d ${vcfs[0]} -o fifo_0 &

    tail +$((n_lines)) fifo_0 | tee fifo_to_paste_0 | cut -f1-5,9 | md5sum > md5sum_0 &

    for vcf in "${vcfs[@]:1}"; do
      fifo_name="fifo_$i"
      mkfifo "$fifo_name"

      fifo_name_to_md5="fifo_to_md5_$i"
      mkfifo "$fifo_name_to_md5"

      fifo_name_to_paste="fifo_to_paste_$i"
      mkfifo "$fifo_name_to_paste"
      fifos_to_paste+=("$fifo_name_to_paste")

      file_name_md5sum="md5sum_$i"
      md5sums+=("$file_name_md5sum")
      n_lines=$(bcftools view -h --no-version $vcf | awk '!/^#CHROM/' | wc -l | cut -d' ' -f1)

      bgzip -d ${vcf} -o "$fifo_name" &
      tail +$((n_lines)) "$fifo_name" | tee "$fifo_name_to_md5" | cut -f 10- > "$fifo_name_to_paste" &
      cut -f1-5,9 "$fifo_name_to_md5" | md5sum > "$file_name_md5sum" &

      ((i++))
    done

    mkfifo fifo_to_cat

    paste fifo_to_paste_0 "${fifos_to_paste[@]}" | tee fifo_to_cat | awk 'NR % 5000000 == 0' | cut -f 1-5 &


    cat header.vcf fifo_to_cat | bgzip -o ~{output_vcf_basename}.vcf.gz

    for md5sum_file in "${md5sums[@]}"; do
    diff <(cat md5sum_0) <(cat $md5sum_file) >> /dev/null || (echo "Fields 1-5,9 do not match for $md5sum_file" && exit 1)
    done

    for fifo in fifo_*; do
    rm $fifo
    done

  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/bcftools_bgzip:beagle_imputation_v1.0.0"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_vcf = "~{output_vcf_basename}.vcf.gz"
  }
}

task QuerySampleChunkedVcfForReannotation {
  input {
    File vcf
    Int disk_size_gb = ceil(2 * size(vcf, "GiB")) + 10
    Int mem_gb = 4
    Int cpu = 1
    Int preemptible = 3
  }
  String vcf_basename = basename(vcf)

  command <<<
    set -euo pipefail

    bcftools query -f '%CHROM,%POS,%REF,%ALT[,%DS,%AP1,%AP2]\n' ~{vcf} | gzip -c > query_tbl.csv.gz
    bcftools query -l ~{vcf} | wc -l | awk '{print $1}' > n_samples.txt
  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/samtools-suite:v1.1"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_query_file = "query_tbl.csv.gz"
    Int n_samples = read_int("n_samples.txt")
  }
}

task RecalculateDR2AndAFChunked {
  input {
    File query_file
    Int n_samples
    Int disk_size_gb = ceil(2 * size(query_file, "GiB")) + 10
    Int mem_gb = 6
    Int cpu = 1
    Int chunksize = 30000
    Int preemptible = 3
  }

  command <<<
    set -euo pipefail

    python3 << EOF
    import pandas as pd
    import numpy as np

    # pandas spits out a view vs copy warning a lot of times which isn't relevant to this code
    pd.options.mode.chained_assignment = None  # default='warn'

    ds_dict = {f'sample_{i}_DS': 'float' for i in range(~{n_samples})}
    ap1_dict = {f'sample_{i}_AP1': 'float' for i in range(~{n_samples})}
    ap2_dict = {f'sample_{i}_AP2': 'float' for i in range(~{n_samples})}
    dtypes_dict = {**ds_dict, **ap1_dict, **ap2_dict}

    csv_names = ["CHROM","POS","REF","ALT"] + [f'sample_{i}_{col}' for i in range(~{n_samples}) for col in ("DS", "AP1", "AP2")]

    out_annotation_dfs = []
    for chunk in pd.read_csv("~{query_file}", names=csv_names, dtype=dtypes_dict, na_values=".", chunksize = ~{chunksize}, lineterminator="\n"):
      # get sample level annotaions necessary for AF and DR2 calculations
      dosages = chunk[[f'sample_{i}_DS' for i in range(~{n_samples})]].to_numpy()
      ap1 = chunk[[f'sample_{i}_AP1' for i in range(~{n_samples})]].to_numpy()
      ap2 = chunk[[f'sample_{i}_AP2' for i in range(~{n_samples})]].to_numpy()

      # sum dosages
      sum_ds = np.sum(dosages, axis=1)

      # sum AP1, AP2
      sum_ap1 = np.sum(ap1, axis=1)
      sum_ap1_squared = np.sum(ap1**2, axis=1)
      sum_ap2 = np.sum(ap2, axis=1)
      sum_ap2_squared = np.sum(ap2**2, axis=1)

      # values to annotate the vcf with
      chunk_annotations = chunk[["CHROM","POS","REF","ALT"]]
      chunk_annotations["SUM_DS"] = sum_ds
      chunk_annotations["SUM_AP1"] = sum_ap1
      chunk_annotations["SUM_AP1_SQUARED"] = sum_ap1_squared
      chunk_annotations["SUM_AP2"] = sum_ap2
      chunk_annotations["SUM_AP2_SQUARED"] = sum_ap2_squared
      chunk_annotations["NUM_SAMPLES"] = ~{n_samples}
      out_annotation_dfs.append(chunk_annotations)

    annotations_df = pd.concat(out_annotation_dfs)
    annotations_df.to_csv("chunked_annotations.tsv", sep="\t", index=False, header=False)
    EOF

  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/samtools-suite:v1.1"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_summary_file = "chunked_annotations.tsv"
  }
}

task AggregateChunkedDR2AndAF {
  input {
    Array[File] sample_chunked_annotation_files
    Int disk_size_gb = ceil(2 * size(sample_chunked_annotation_files, "GiB")) + 10
    Int mem_gb = 6
    Int cpu = 1
    Int preemptible = 3
    Int line_chunk_size = 100000
  }

  command <<<
    set -euo pipefail

    echo "$(date) - recomputing annotations"

    python3 << EOF
    import pandas as pd
    import numpy as np
    import gc

    # pandas spits out a view vs copy warning a lot of times which isn't relevant to this code
    pd.options.mode.chained_assignment = None  # default='warn'

    dtypes_dict = {f'{col}': 'float' for col in ("SUM_DS", "SUM_AP1", "SUM_AP1_SQUARED", "SUM_AP2", "SUM_AP2_SQUARED")}
    dtypes_dict["NUM_SAMPLES"] = "int"

    file_paths = ["~{sep='", "' sample_chunked_annotation_files}"]

    # Get number of lines in first file since all files should have the same number of lines
    num_lines = sum(1 for _ in open(file_paths[0]))

    all_aggregated_results = []
    chunk_start = 0
    line_chunk_size = ~{line_chunk_size}

    while chunk_start < num_lines:
      chunked_dfs = []
      for file_path in file_paths:
        chunked_df = pd.read_csv(file_path, sep="\t", header=None, names=["CHROM", "POS", "REF", "ALT", "SUM_DS", \
        "SUM_AP1", "SUM_AP1_SQUARED", "SUM_AP2", "SUM_AP2_SQUARED", "NUM_SAMPLES"], dtype=dtypes_dict, \
        skiprows=chunk_start, nrows=line_chunk_size)
        chunked_dfs.append(chunked_df)

      combined_chunk_df = pd.concat(chunked_dfs)

      # Group by CHROM, POS, REF, ALT and aggregate SUM_DS, SUM_AP1, SUM_AP2
      aggregated_df = combined_chunk_df.groupby(["CHROM", "POS", "REF", "ALT"]).agg({
          "SUM_DS": "sum",
          "SUM_AP1": "sum",
          "SUM_AP1_SQUARED": "sum",
          "SUM_AP2": "sum",
          "SUM_AP2_SQUARED": "sum",
          "NUM_SAMPLES": "sum"
      }).reset_index()

      # Aggregated NUM_SAMPLES should be the same across all rows, so we can take the first value
      num_samples = aggregated_df["NUM_SAMPLES"].iloc[0]

      # Calculate AF
      af = aggregated_df["SUM_DS"] / (2 * num_samples)
      aggregated_df["AF"] = np.round(af, 4)

      # Calculate DR2
      sum_squared_ap1_ap2 = aggregated_df["SUM_AP1_SQUARED"] + aggregated_df["SUM_AP2_SQUARED"]
      sum_ap1_ap2 = aggregated_df["SUM_AP1"] + aggregated_df["SUM_AP2"]

      denominator = (2 * num_samples * sum_ap1_ap2) - (sum_ap1_ap2)**2

      aggregated_df["DR2"] = np.where(
          (af == 0) | (af == 1) | (denominator == 0), 0.00, np.round(((2 * num_samples * sum_squared_ap1_ap2) - (sum_ap1_ap2**2)) / denominator, 2)
      )

      # Select relevant columns for output and add to final output
      aggregated_df = aggregated_df[["CHROM", "POS", "REF", "ALT", "AF", "DR2"]]
      all_aggregated_results.append(aggregated_df)

      # clean up memory
      del chunked_dfs, combined_chunk_df, aggregated_df
      gc.collect()

      # Move to the next chunk
      chunk_start += line_chunk_size

    # Concatenate all results
    all_aggregated_df = pd.concat(all_aggregated_results, ignore_index=True)

    # Save the aggregated DataFrame to a TSV file
    all_aggregated_df.to_csv("aggregated_annotations.tsv", sep="\t", index=False, header=False)
    EOF

    echo "$(date) - annotations recomputed"

    bgzip aggregated_annotations.tsv
    tabix -s1 -b2 -e2 aggregated_annotations.tsv.gz

    echo "$(date) - done zipping and indexing aggregated annotations"
  >>>

  runtime {
    docker: "us.gcr.io/broad-dsde-methods/samtools-suite:v1.1"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_annotations_file = "aggregated_annotations.tsv.gz"
    File output_annotations_file_index = "aggregated_annotations.tsv.gz.tbi"
  }
}

task RemoveAPAnnotations {
  input {
    File vcf
    File vcf_index
    Int disk_size_gb = ceil(2 * size(vcf, "GiB")) + 10
    Int mem_gb = 4
    Int cpu = 1
    Int preemptible = 3
  }

  String output_base = basename(vcf, ".vcf.gz")

  command <<<
    set -euo pipefail

    echo "$(date) - removing ap1 and ap2 annotations from vcf"
    bcftools annotate --no-version -x FORMAT/AP1,FORMAT/AP2 -Oz -o ~{output_base}.vcf.gz ~{vcf}
  >>>


  runtime {
    docker: "us.gcr.io/broad-dsde-methods/samtools-suite:v1.1"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_vcf = "~{output_base}.vcf.gz"
  }
}

task ReannotateDR2AndAF {
  input {
    File vcf
    File vcf_index
    File annotations_tsv
    File annotations_tsv_index
    Int disk_size_gb = ceil(2 * size(vcf, "GiB") + size(annotations_tsv, "GiB")) + 10
    Int mem_gb = 4
    Int cpu = 1
    Int preemptible = 3
  }

  String output_base = basename(vcf, ".vcf.gz")

  command <<<
    set -euo pipefail

    echo "$(date) - annotating vcf with recalculated af and dr2 annotations"
    bcftools annotate --no-version -a ~{annotations_tsv} -c CHROM,POS,REF,ALT,AF,DR2 -Oz -o ~{output_base}.vcf.gz ~{vcf}

    echo "$(date) - indexing annotated vcf"
    bcftools index -t ~{output_base}.vcf.gz
  >>>


  runtime {
    docker: "us.gcr.io/broad-dsde-methods/samtools-suite:v1.1"
    disks: "local-disk " + disk_size_gb + " HDD"
    memory: mem_gb + " GiB"
    cpu: cpu
    preemptible: preemptible
    maxRetries: 1
    noAddress: true
  }

  output {
    File output_vcf = "~{output_base}.vcf.gz"
    File output_vcf_index = "~{output_base}.vcf.gz.tbi"
  }
}

task GatherVcfsNoIndex {
  input {
    Array[File] input_vcfs
    String output_vcf_basename

    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.6.1.0"
    Int cpu = 2
    Int memory_mb = 10000
    Int disk_size_gb = ceil(3*size(input_vcfs, "GiB")) + 10
  }
  Int command_mem = memory_mb - 1500
  Int max_heap = memory_mb - 1000

  command <<<
    set -e -o pipefail

    gatk --java-options "-Xms~{command_mem}m -Xmx~{max_heap}m" \
    GatherVcfs \
    -I ~{sep=' -I ' input_vcfs} \
    --REORDER_INPUT_BY_FIRST_VARIANT \
    -O ~{output_vcf_basename}.vcf.gz
  >>>
  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} SSD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    maxRetries: 1
    noAddress: true
  }
  output {
    File output_vcf = "~{output_vcf_basename}.vcf.gz"
  }
}

task FilterVcfByDR2 {
  input {
    File vcf
    Float dr2_threshold
    String basename

    Int disk_size_gb = ceil(2*size(vcf, "GiB")) + 10
    Int cpu = 1
    Int memory_mb = 6000
    String gatk_docker = "us.gcr.io/broad-gatk/gatk:4.6.1.0"
  }
  Int command_mem = memory_mb - 1500
  Int max_heap = memory_mb - 1000

  command {
    set -e -o pipefail

    bcftools filter -i 'INFO/DR2 >= ~{dr2_threshold}' -Oz -o ~{basename}.vcf.gz ~{vcf}

    bcftools index -t ~{basename}.vcf.gz
  }
  runtime {
    docker: gatk_docker
    disks: "local-disk ${disk_size_gb} HDD"
    memory: "${memory_mb} MiB"
    cpu: cpu
    preemptible: 3
    maxRetries: 1
    noAddress: true
  }
  parameter_meta {
    vcf: {
           description: "vcf",
           localization_optional: true
         }
  }
  output {
    File output_vcf = "~{basename}.vcf.gz"
    File output_vcf_index = "~{basename}.vcf.gz.tbi"
  }
}

