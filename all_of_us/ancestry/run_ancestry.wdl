version 1.0

# This workflow will train a model and then apply it to a set of samples.  The set of samples do not have any ground
#  truth data.
#
#  Note: This workflow assumes that the training set is public (like HGDP) and that the vcf shards might need a service
#   account to run.
workflow run_ancestry {
    input {

        # The high quality sites, as sites-only, VCF.  These are generated by determine_hq_sites_intersection.wdl.
        #  I.e. the high-quality sites taking into account the intersection between training and test data
        File hq_variants_intersection
        File hq_variants_intersection_idx

        # The data we wish to obtain ancestry as a single merged VCF
        File merged_vcf_shards
        File merged_vcf_shards_idx

        # The training data at the high quality sites (using the same sites as hq_variants_intersection)
        #  This is the full version (with samples) of the hq_variants_intersection, which is sites only.
        File filtered_training_set
        File filtered_training_set_idx

        # The HGDP sample metadata file
        File? hgdp_metadata_file_in

        String final_output_prefix

        # Prediction probability cutoff for classifying ancestry as "oth" ("Other")
        Float? other_cutoff_in

        # How many PCs to use for training, etc.
        Int num_pcs=16
    }

    File hgdp_metadata_file = select_first([hgdp_metadata_file_in, "gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/metadata_and_qc/gnomad_meta_updated.tsv"])
    Float other_cutoff = select_first([other_cutoff_in, 0.75])
    String pipeline_version = "aou_9.0.0"

    # Train the model on the intersection sites (full version that includes the samples)
    call create_hw_pca_training {
        input:
            full_bgz=filtered_training_set,
            full_bgz_index=filtered_training_set_idx,
            hgdp_metadata_file=hgdp_metadata_file,
            final_output_prefix=final_output_prefix,
            num_pcs=num_pcs
    }

    call call_ancestry {
        input:
            input_vcf = merged_vcf_shards,
            input_vcf_idx = merged_vcf_shards_idx,
            training_vcf = filtered_training_set,
            training_vcf_idx = filtered_training_set_idx,
            loadings_training_ht_tar_gz = create_hw_pca_training.loadings_training_ht_tar_gz,
            training_pca_labels_ht_tar_gz = create_hw_pca_training.training_pca_labels_ht_tar_gz,
            output_prefix = final_output_prefix,
            other_cutoff_in = other_cutoff
    }

    call plot_ancestry {
        input:
            results_tsv = call_ancestry.results_tsv,
            results_ht = call_ancestry.results_ht,
            output_prefix = final_output_prefix
    }

    output {
        File results_tsv = call_ancestry.results_tsv
        File results_ht = call_ancestry.results_ht
        File results_loadings_ht = call_ancestry.results_loadings_ht
        File pred_plot = plot_ancestry.pred_plot
        File pred_oth_plot = plot_ancestry.pred_oth_plot
        File training_pca_labels_ht_tsv = create_hw_pca_training.training_pca_labels_ht_tsv
        File eigenvalues_txt = create_hw_pca_training.eigenvalues_txt
        File classifier_pkl = call_ancestry.classifier_pkl
    }
}

task create_hw_pca_training {
    input {
        File full_bgz
        File full_bgz_index
        File hgdp_metadata_file
        String final_output_prefix
        Int num_pcs
    }
    parameter_meta {
        full_bgz: {localization_optional: true}
        full_bgz_index: {localization_optional: true}
    }
    command <<<
        set -e
        python3 <<EOF

        import pandas as pd
        import numpy as np
        import hail as hl
        hl.init(default_reference='GRCh38', idempotent=True)

        import pandas as pd

        # Read in metadata for ALL samples
        metadata_pd = pd.read_csv("~{hgdp_metadata_file}", sep="\t")

        def get_PCA_scores(vcf_bgz:str):
            v = hl.import_vcf(vcf_bgz, force_bgz=True)
            eigenvalues, scores, loadings = hl.hwe_normalized_pca(v.GT, k=~{num_pcs}, compute_loadings=True)
            return eigenvalues, scores, loadings

        def collapse_fin_to_eur(pop):
            if pop=='fin' or pop=='nfe':
                return 'eur'
            else:
                return pop

        eigenvalues_training, scores_training, loadings_training = get_PCA_scores("~{full_bgz}")

        # Apply any custom processing to the population labels from the training data
        pop_label_pd = metadata_pd[['s', 'project_meta.project_pop']]
        pop_label_pd['pop_label'] = metadata_pd['project_meta.project_pop'].apply(collapse_fin_to_eur)

        # Join the labels to the training PCA feature set.
        pop_label_ht = hl.Table.from_pandas(pop_label_pd).key_by('s')
        pop_label_ht.describe()
        training_pca_ht = scores_training.join(pop_label_ht, how='inner')

        # Write out the training pca and loadings information
        training_pca_ht.write('training_pca.ht')
        loadings_training.write('loadings_training.ht')
        pop_label_ht.write('pop_label.ht')

        # Write out the training_pca as a tsv.
        training_pca_ht_export_tsv = training_pca_ht.flatten()
        training_pca_ht_export_tsv.export("~{final_output_prefix}_training_pca.tsv")

        # Write out the eigenvalues simple text file, no header, one value per line in text.
        eigenvalues_str = "\n".join([str(e) for e in eigenvalues_training])
        fp = open("~{final_output_prefix}_eigenvalues.txt", 'w')
        fp.write(eigenvalues_str)
        fp.flush()
        fp.close()
        EOF

        # tar the Hail Tables for output in cromwell
        tar zcvf ~{final_output_prefix}_training_pca.ht.tar.gz training_pca.ht
        tar zcvf ~{final_output_prefix}_loadings_training.ht.tar.gz loadings_training.ht
        tar zcvf ~{final_output_prefix}_pop_label.ht.tar.gz pop_label.ht
    >>>

    output {
        File training_pca_labels_ht_tar_gz = "~{final_output_prefix}_training_pca.ht.tar.gz"
        File training_pca_labels_ht_tsv = "~{final_output_prefix}_training_pca.tsv"
        File loadings_training_ht_tar_gz = "~{final_output_prefix}_loadings_training.ht.tar.gz"
        File pop_label_ht_tar_gz = "~{final_output_prefix}_pop_label.ht.tar.gz"
        File eigenvalues_txt = "~{final_output_prefix}_eigenvalues.txt"
    }

    runtime {
        docker: "hailgenetics/hail:0.2.67"
        memory: "123 GB"
        cpu: "4"
        disks: "local-disk 500 HDD"
    }
}

task call_ancestry {
    input {
        File input_vcf
        File input_vcf_idx
        File training_vcf
        File training_vcf_idx
        File loadings_training_ht_tar_gz
        File training_pca_labels_ht_tar_gz
        String output_prefix
        Float? other_cutoff_in
    }

    String output_results_tsv = basename(output_prefix) + ".ancestry_preds.tsv"
    String output_results_ht = basename(output_prefix) + ".ancestry_preds.ht"
    String output_results_loadings_ht = basename(output_prefix) + "_loadings.ht"
    Float other_cutoff = select_first([other_cutoff_in, 0.75])

    parameter_meta {
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        training_vcf: {localization_optional: true}
        training_vcf_idx: {localization_optional: true}
    }

    command <<<
        set -e
        tar zxvf ~{loadings_training_ht_tar_gz}
        tar zxvf ~{training_pca_labels_ht_tar_gz}

        python3 <<EOF
        import pandas as pd
        import numpy as np
        import hail as hl
        import pickle

        # Add more RAM
        spark_conf_more_ram = dict()
        spark_conf_more_ram["spark.executor.memory"] = "24g"
        spark_conf_more_ram["spark.driver.memory"] = "24g"

        hl.init(default_reference='GRCh38', idempotent=True, spark_conf=spark_conf_more_ram)

        # Initialize the RF classifier.
        from sklearn.ensemble import RandomForestClassifier
        import numpy as np
        import pandas as pd

        training_pca_ht = hl.methods.read_table('training_pca.ht')
        loadings_training = hl.methods.read_table('loadings_training.ht')

        # Set up loadings from the training data to include the AF
        training_mt = hl.import_vcf("~{training_vcf}", force_bgz=True)
        training_mt = training_mt.annotate_rows(af2=hl.agg.mean(training_mt.GT.n_alt_alleles()) / 2)
        loadings_training = loadings_training.annotate(af=training_mt.rows()[loadings_training.key].af2)
        loadings_training.write("~{output_results_loadings_ht}", overwrite=True)

        reduced_training_pca_ht = training_pca_ht.filter((training_pca_ht['pop_label'] != "oth"))

        # Train the classifier
        clf = RandomForestClassifier(random_state=337337)
        X = reduced_training_pca_ht.scores.collect()
        y = reduced_training_pca_ht['pop_label'].collect()
        clf.fit(X, y)

        # Project the samples into principal component space from the training data.
        testing_mt = hl.import_vcf("~{input_vcf}", force_bgz=True)
        testing_ht = hl.experimental.pc_project(testing_mt.GT, loadings_training.loadings, loadings_training.af)
        testing_ht.write("~{output_results_ht}", overwrite=True)

        # Run the samples through the classifier.  Get the predictions and the probabilities.
        test_X = testing_ht.scores.collect()
        preds = clf.predict(test_X)
        probs = clf.predict_proba(test_X)

        # Convert the probabilities into a list, so that when you save the data frame, commas are included.  If this
        #  does not happen, Hail will choke attempting to read this field.
        probs_list = probs.tolist()

        # Apply the "other (oth)" ancestry prediction on a data frame row.  Note that the field names are hardcoded.
        #  Takes in the probabilities and just looks to see the confidence in the classifier of a given prediction.
        #  If not confident, make the updated prediction "oth"
        def predict_other(r):
            pred = r.ancestry_pred
            probs = r.probabilities
            probs = [float(x) for x in probs]
            if max(probs) > ~{other_cutoff}:
                return pred
            else:
                return "oth"

        # Save the results
        results_df = pd.DataFrame({'research_id': testing_ht.s.collect(), "ancestry_pred": preds, "probabilities":probs_list, "pca_features": test_X})
        results_df['ancestry_pred_other'] = results_df[["ancestry_pred", "probabilities"]].apply(predict_other, axis=1)
        results_df.to_csv("~{output_results_tsv}", sep="\t", index=False)

        # Save the classifier (pickle)
        pickle.dump(clf, open("~{output_prefix}_rf_classifier.pkl", "wb"))

        EOF

        # tar the Hail Tables for output in cromwell
        tar zcvf ~{output_results_ht}.tar.gz ~{output_results_ht}
        tar zcvf ~{output_results_loadings_ht}.tar.gz ~{output_results_loadings_ht}

    >>>
    output {
        File results_tsv = "~{output_results_tsv}"
        File results_ht = "~{output_results_ht}.tar.gz"
        File results_loadings_ht = "~{output_results_loadings_ht}.tar.gz"
        File classifier_pkl = "~{output_prefix}_rf_classifier.pkl"
    }
    runtime {
        docker: "hailgenetics/hail:0.2.67"
        memory: "240 GB"
        cpu: "4"
        disks: "local-disk 700 HDD"
    }
}

# Note: This task could be optimized if we simply want to generate pngs.  However, the bokeh plotting is more
#  interactive.  There is an issue where Hail cannot convert pandas data frames that have array values in a column (e.g.
#  probabilities or pca_features.)
task plot_ancestry {
    input {
        File results_tsv
        File results_ht
        String output_prefix
    }

    # Note that this variable depends on the previous task (call_ancestry)
    String results_ht_filename = sub(basename(results_ht), ".tar.gz","")

    command <<<
        set -e
        tar zxvf ~{results_ht}

        python3 <<EOF
        import pandas as pd
        import numpy as np
        import hail as hl
        hl.init(default_reference='GRCh38', idempotent=True)
        from bokeh.io import output_file
        from bokeh.io import show

        testing_ht = hl.methods.read_table('~{results_ht_filename}')
        results_df = pd.read_csv("~{results_tsv}", sep="\t")
        results_df = results_df.astype({'research_id': str})

        # Join the predictions to to the testing table.
        preds_ht = hl.Table.from_pandas(results_df[['research_id', 'ancestry_pred', 'ancestry_pred_other']], key='research_id')
        annotated_reduced_testing_ht = testing_ht.annotate(preds=preds_ht[testing_ht.s].ancestry_pred)
        annotated_reduced_testing_ht = annotated_reduced_testing_ht.annotate(preds_oth=preds_ht[annotated_reduced_testing_ht.s].ancestry_pred_other)

        # Create a dictionary that maps ancestry three letter string to a more readable description

        ancestries = ["afr", "amr","eas", "eur", "mid", "oth", "sas"]

        pred_label_dict_no_N = {
            "afr": "African", "amr":"Latino/Admixed American","eas":"East Asian", "eur": "European", "mid": "Middle Eastern", "oth": "Other", "sas": "South Asian"
        }

        def create_ancestry_dict_with_N(relevant_column:str, df:pd.DataFrame):
            pred_label_dict = dict()
            for a in ancestries:
                is_a = df[relevant_column] == a
                if is_a.sum() == 0:
                    continue
                pred_label_dict[a] = f'{pred_label_dict_no_N[a]} (N={is_a.sum()})'
            return pred_label_dict

        pred_label_dict_oth = create_ancestry_dict_with_N('ancestry_pred_other', results_df)
        pred_label_dict_no_oth = create_ancestry_dict_with_N('ancestry_pred', results_df)

        d_oth = hl.literal(pred_label_dict_oth)
        d_no_oth = hl.literal(pred_label_dict_no_oth)

        annotated_reduced_testing_ht = annotated_reduced_testing_ht.annotate(preds_label=d_no_oth[annotated_reduced_testing_ht['preds']])
        annotated_reduced_testing_ht = annotated_reduced_testing_ht.annotate(preds_label_oth=d_oth[annotated_reduced_testing_ht['preds_oth']])

        #
        # Perform the actual plotting.
        #
        from bokeh.palettes import Category10
        from bokeh.models.mappers import CategoricalColorMapper

        factors = sorted(pred_label_dict_no_oth.values())
        print(factors)
        color_mapper = CategoricalColorMapper(palette=Category10[10], factors=factors)

        output_file("~{output_prefix}.preds.html")
        tmp = hl.plot.scatter(annotated_reduced_testing_ht.scores[0], annotated_reduced_testing_ht.scores[1], size=2, label=dict({"pred_ancestry":annotated_reduced_testing_ht['preds_label']}), hover_fields=dict({"id":annotated_reduced_testing_ht['s']}),
            title="Principal Components 1 and 2", colors=color_mapper)
        show(tmp)

        output_file("~{output_prefix}.preds_oth.html")
        tmp2 = hl.plot.scatter(annotated_reduced_testing_ht.scores[0], annotated_reduced_testing_ht.scores[1], size=2, label=dict({"pred_ancestry":annotated_reduced_testing_ht['preds_label_oth']}), hover_fields=dict({"id":annotated_reduced_testing_ht['s']}),
            title="Principal Components 1 and 2", colors=color_mapper)
        show(tmp2)

        EOF

    >>>

    output {
        File pred_plot = "~{output_prefix}.preds.html"
        File pred_oth_plot = "~{output_prefix}.preds_oth.html"
    }

    runtime {
        docker: "hailgenetics/hail:0.2.67"
        memory: "7 GB"
        cpu: "4"
        disks: "local-disk 100 HDD"
    }
}