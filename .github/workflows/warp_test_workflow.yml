name: Reusable WARP Test Workflow

on:
  workflow_call:
    inputs:
      pipeline_name:
        required: true
        type: string
        description: 'Name of the pipeline to test'
      dockstore_pipeline_name:
        required: true
        type: string
        description: 'Name of the pipeline in Dockstore'
      pipeline_dir:
        required: true
        type: string
        description: 'Directory containing the pipeline'
      use_call_cache:
        required: false
        type: string
        default: 'true'
        description: 'Use call cache'
      update_truth:
        required: false
        type: string
        default: 'false'
        description: 'Update truth files'
      test_type:
        required: false
        type: string
        default: 'Plumbing'
        description: 'Type of test (Plumbing or Scientific)'
      truth_branch:
        required: false
        type: string
        default: 'master'
        description: 'Branch for truth files'

    secrets:
      PDT_TESTER_SA_B64:
        required: true
      DOCKSTORE_TOKEN:
        required: true

env:
  TESTING_WORKSPACE: WARP Tests
  WORKSPACE_NAMESPACE: warp-pipelines
  SA_JSON_B64: ${{ secrets.PDT_TESTER_SA_B64 }}
  USER: pdt-tester@warp-pipeline-dev.iam.gserviceaccount.com

jobs:
  test_pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
      actions: write

    steps:
      # Step 1: Checkout code
      # Purpose: Clones the repository code at the specified reference
      - uses: actions/checkout@v3
        with:
          ref: ${{ github.ref }}

        # Step 2: Setup Python
        # Purpose: Installs Python 3.11 for running pipeline scripts
      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Step 3: Install Dependencies
      # Purpose: Installs required Python packages for the pipeline
      - name: Install dependencies
        run: |
          cd scripts/firecloud_api/
          pip install -r requirements.txt

      # Step 4: Set Branch Name
      # Purpose: Determines and sets the correct branch name for either PR or direct commits
      - name: Set Branch Name
        id: set_branch
        run: |
          if [ -z "${{ github.head_ref }}" ]; then
            echo "Branch name is missing, using ${GITHUB_REF##*/}"
            echo "BRANCH_NAME=${GITHUB_REF##*/}" >> $GITHUB_ENV
          else
            echo "Branch name from PR: ${{ github.head_ref }}"
            echo "BRANCH_NAME=${{ github.head_ref }}" >> $GITHUB_ENV
          fi

      # Step 5: Set Test Type
      # Purpose: Determines and sets the correct test type based on the branch name
      - name: Set Test Type
        id: set_test_type
        run: |
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            # For PRs, set based on target branch
            if [ "${{ github.base_ref }}" == "master" ]; then
              # If PR is targeting master branch, run Scientific tests
              echo "testType=Scientific" >> $GITHUB_ENV
              echo "testType=Scientific"
            else
              # If PR targets any other branch (develop, staging), run Plumbing tests
              echo "testType=Plumbing" >> $GITHUB_ENV
              echo "testType=Plumbing"
            fi
          else
            # For manual workflow runs (workflow_dispatch)
            echo "testType=${{ inputs.test_type }}" >> $GITHUB_ENV
            echo "testType=${{ inputs.test_type }}"
          fi

      # Step 6: Create Method Configuration
      # Purpose: Sets up the testing configuration in Terra workspace
      - name: Create new method configuration
        run: |
          # Wait 5.5 minutes for Dockstore to update
          echo "Waiting for Dockstore to update..."
          sleep 330
          
          echo "Creating new method configuration for branch: $BRANCH_NAME"

          METHOD_CONFIG_NAME=$(python3 scripts/firecloud_api/firecloud_api.py \
            create_new_method_config \
            --workspace-namespace $WORKSPACE_NAMESPACE \
            --workspace-name "$TESTING_WORKSPACE" \
            --pipeline_name "${{ inputs.pipeline_name }}" \
            --branch_name "$BRANCH_NAME" \
            --test_type "$testType" \
            --sa-json-b64 "$SA_JSON_B64" \
            --user "$USER")

          echo "METHOD_CONFIG_NAME=$METHOD_CONFIG_NAME" >> $GITHUB_ENV

      # Step 7: Cancel Previous Runs
      # Purpose: Cancels previous GHA workflows from the same branch (regardless of plumbing or scientific test type)
      # to avoid running multiple tests at the same time
      - name: Cancel Previous GHA Runs
        uses: styfle/cancel-workflow-action@0.11.0
        with:
          access_token: ${{ github.token }}
          all_but_latest: true
          ignore_sha: true

      # Step 8: Cancel Previous Terra Submissions
      # Purpose: Abort previous Terra submissions from the same branch to avoid running multiple tests at the same time
      # Will not abort a Terra submission if it is a scientific test
      - name: Cancel Previous Terra Submissions
        if: ${{ !contains(env.METHOD_CONFIG_NAME, '_Scientific_') }}
        run: |
          python3 scripts/firecloud_api/firecloud_api.py \
            --workspace-namespace "${{ env.WORKSPACE_NAMESPACE }}" \
            --workspace-name "${{ env.TESTING_WORKSPACE }}" \
            --pipeline_name "${{ inputs.pipeline_name }}" \
            --branch_name "${{ env.BRANCH_NAME }}" \
            --sa-json-b64 "${{ secrets.PDT_TESTER_SA_B64 }}" \
            --user "${{ env.USER }}" \
            --test_type "$testType" \
            cancel_old_submissions

      # Step 9: Handle Git Commit Hash
      # Purpose: Gets the correct Github commit hash for version tracking
      - name: Determine Github Commit Hash
        id: determine_github_commit_hash
        run: |
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Using github.sha for manually triggered workflow."
            echo "GITHUB_COMMIT_HASH=${{ github.sha }}" >> $GITHUB_ENV
          elif [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "Using github.event.pull_request.head.sha for PR-triggered workflow."
            echo "GITHUB_COMMIT_HASH=${{ github.event.pull_request.head.sha }}" >> $GITHUB_ENV
          else
            echo "Unsupported event type: ${{ github.event_name }}"
            exit 1
          fi

      # Step 10: Compare Hashes
      # Purpose: Compares the Dockstore and Github commit hashes to ensure they match
      - name: Compare Dockstore and Github Commit Hashes with Retry
        id: compare_hashes
        run: |

          MAX_WAIT_TIME=$((15 * 60)) # 15 minutes in seconds
          WAIT_INTERVAL=60          # 1 minute in seconds
          TOTAL_WAITED=0

          echo "Starting hash comparison with retry mechanism..."

          while [ $TOTAL_WAITED -lt $MAX_WAIT_TIME ]; do
            echo "Fetching Dockstore Commit Hash..."
            DOCKSTORE_COMMIT_HASH=$(python scripts/dockstore_api/fetch_dockstore_commit.py \
              $DOCKSTORE_TOKEN \
              ${{ inputs.dockstore_pipeline_name }} \
              $BRANCH_NAME)
            echo "Fetched Dockstore Commit Hash: $DOCKSTORE_COMMIT_HASH"

            echo "GitHub Commit Hash: $GITHUB_COMMIT_HASH"

            if [ "$DOCKSTORE_COMMIT_HASH" == "$GITHUB_COMMIT_HASH" ]; then
              echo "Success: The Dockstore Commit Hash matches the GitHub Commit Hash."
              exit 0
            else
              echo "Mismatch found: $DOCKSTORE_COMMIT_HASH != $GITHUB_COMMIT_HASH"
              echo "Retrying in $WAIT_INTERVAL seconds..."
              sleep $WAIT_INTERVAL
              TOTAL_WAITED=$((TOTAL_WAITED + WAIT_INTERVAL))
            fi
          done

          echo "Error: The Dockstore Commit Hash does not match the GitHub Commit Hash after 15 minutes of retries!"
          
          exit 1
        env:
          GITHUB_COMMIT_HASH: ${{ env.GITHUB_COMMIT_HASH }}
          DOCKSTORE_TOKEN: ${{ secrets.DOCKSTORE_TOKEN }}

      # Step 11: Run Tests
      # Purpose: Main testing step - runs the pipeline and collects results
      - name: Update test inputs, Upload to Terra, Submit, Monitor and Retrieve Outputs
        run: |
          UPDATE_TRUTH="${{ inputs.update_truth }}"
          USE_CALL_CACHE="${{ inputs.use_call_cache }}"
          TRUTH_BRANCH="${{ inputs.truth_branch }}"
          CURRENT_TIME=$(date +"%Y-%m-%d-%H-%M-%S")
          MAX_RETRIES=2
          RETRY_DELAY=300  # 300 seconds = 5 minutes
          # Initialize variables to aggregate statuses and outputs
          ALL_WORKFLOW_STATUSES="Workflow ID | Status"$'\n'"--- | ---"
          ALL_OUTPUTS=""
          declare -a SUBMISSION_IDS
          declare -A WORKFLOW_STATUSES
          declare -A SUBMISSION_ID_TO_FILENAME

          OVERALL_SUCCESS=true  

          # Convert UPDATE_TRUTH and USE_CALL_CACHE to a boolean-friendly format ("true" -> true, "false" -> false)
          if [ "$UPDATE_TRUTH" = "true" ]; then
              UPDATE_TRUTH_BOOL=true
          else
              UPDATE_TRUTH_BOOL=false
          fi

          if [ "$USE_CALL_CACHE" == "true" ]; then
              USE_CALL_CACHE_BOOL=true
          else
              USE_CALL_CACHE_BOOL=false
          fi

          TEST_TYPE="${{ env.testType }}"
          INPUTS_DIR="${{ inputs.pipeline_dir }}/test_inputs/$TEST_TYPE"
          echo "Running tests with test type: $TEST_TYPE"
          
          RESULTS_PATH="gs://pd-test-results/${{ inputs.dockstore_pipeline_name }}/results/$CURRENT_TIME"


          # Submit all jobs first and store their submission IDs
          for input_file in "$INPUTS_DIR"/*.json; do
            test_input_file=$(python3 scripts/firecloud_api/UpdateTestInputs.py \
                --results_path "$RESULTS_PATH" \
                --inputs_json "$input_file" \
                --update_truth "$UPDATE_TRUTH_BOOL" \
                --branch_name "$TRUTH_BRANCH" \
                --dockstore_pipeline_name "${{ inputs.dockstore_pipeline_name }}" )
            echo "Uploading the test input file: $test_input_file"

            # Create the submission_data.json file for this input_file
            input_file_filename=$(basename $input_file)
            SUBMISSION_DATA_FILE="submission_data.json"
            printf '{
              "methodConfigurationNamespace": "%s",
              "methodConfigurationName": "%s_%s_%s",
              "useCallCache": %s,
              "deleteIntermediateOutputFiles": false,
              "useReferenceDisks": true,
              "memoryRetryMultiplier": 1.2,
              "workflowFailureMode": "NoNewCalls",
              "userComment": "%s",
              "ignoreEmptyOutputs": false
            }' "$WORKSPACE_NAMESPACE" "${{ inputs.pipeline_name }}" "$TEST_TYPE" "$BRANCH_NAME" "$USE_CALL_CACHE_BOOL" "$input_file_filename" > "$SUBMISSION_DATA_FILE"
          
            echo "Created submission data file: $SUBMISSION_DATA_FILE"
            cat "$SUBMISSION_DATA_FILE"
          
            # Upload the test inputs to Terra
            python3 scripts/firecloud_api/firecloud_api.py \
                upload_test_inputs \
                --workspace-namespace $WORKSPACE_NAMESPACE \
                --workspace-name "$TESTING_WORKSPACE" \
                --pipeline_name "${{ inputs.pipeline_name }}" \
                --test_input_file "$test_input_file" \
                --branch_name "$BRANCH_NAME" \
                --sa-json-b64 "$SA_JSON_B64" \
                --test_type "$TEST_TYPE" \
                --user "$USER"

            attempt=1
            while [ $attempt -le $MAX_RETRIES ]; do
              SUBMISSION_ID=$(python3 scripts/firecloud_api/firecloud_api.py submit_job \
               --workspace-namespace "$WORKSPACE_NAMESPACE" \
               --workspace-name "$TESTING_WORKSPACE" \
               --sa-json-b64 "$SA_JSON_B64" \
               --user "$USER" \
               --submission_data_file "$SUBMISSION_DATA_FILE")

              echo "Submission ID: $SUBMISSION_ID"
          
              if [[ "$SUBMISSION_ID" == *"404"* || -z "$SUBMISSION_ID" ]]; then
                  echo "Error in submission, retrying in $RETRY_DELAY seconds..."
                  ((attempt++))
                  if [ $attempt -gt $MAX_RETRIES ]; then
                      echo "Max retries reached. Exiting..."
                      exit 1
                  fi
                  sleep $RETRY_DELAY
                  continue
              fi

              echo "Submission successful. Submission ID: $SUBMISSION_ID"
              SUBMISSION_ID_TO_FILENAME["$SUBMISSION_ID"]="$input_file_filename"          
              SUBMISSION_IDS+=("$SUBMISSION_ID")
              break
            done
          done
          
          echo "All jobs have been submitted. Starting to poll for statuses..."

          # Poll for statuses of all jobs
          for SUBMISSION_ID in "${SUBMISSION_IDS[@]}"; do
            attempt=1
            while [ $attempt -le $MAX_RETRIES ]; do
              echo "Polling for Submission ID: $SUBMISSION_ID"
              RESPONSE=$(python3 scripts/firecloud_api/firecloud_api.py poll_job_status \
                  --submission_id "$SUBMISSION_ID" \
                  --sa-json-b64 "$SA_JSON_B64" \
                  --user "$USER" \
                  --workspace-namespace "$WORKSPACE_NAMESPACE" \
                  --workspace-name "$TESTING_WORKSPACE")
          
              if [ -z "$RESPONSE" ]; then
                  echo "Failed to retrieve Workflow IDs for submission: $SUBMISSION_ID"
                  OVERALL_SUCCESS=false
                  ((attempt++))
                  if [ $attempt -gt $MAX_RETRIES ]; then
                      echo "Max retries reached. Exiting..."
                      exit 1
                  fi
                  sleep $RETRY_DELAY
                  continue
              fi
          
              WORKFLOW_STATUSES_FOR_SUBMISSION=$(echo "$RESPONSE" | jq -r 'to_entries | map(.key + " | " + .value) | .[]')
              WORKFLOW_STATUSES["$SUBMISSION_ID"]="$WORKFLOW_STATUSES_FOR_SUBMISSION"
          
              # Check if any workflow failed or errored
              FAILED_WORKFLOWS=$(echo "$RESPONSE" | jq -r 'to_entries | .[] | select(.value == "Failed" or .value == "Aborted" or .value == "Aborting") | .key')
              if [ ! -z "$FAILED_WORKFLOWS" ]; then
                  echo "Failed workflows detected:"
                  echo "$FAILED_WORKFLOWS"
                  OVERALL_SUCCESS=false
              fi
          
              # retrieve workflow outputs
              echo "Retrieving workflow outputs for Submission ID: $SUBMISSION_ID..."
          
              for WORKFLOW_ID in $(echo "$RESPONSE" | jq -r 'keys[]'); do
                   WORKFLOW_OUTPUT=$(python3 scripts/firecloud_api/firecloud_api.py get_workflow_outputs \
                    --user "$USER" \
                    --sa-json-b64 "$SA_JSON_B64" \
                    --submission_id "$SUBMISSION_ID" \
                    --workspace-namespace $WORKSPACE_NAMESPACE \
                    --workspace-name "$TESTING_WORKSPACE" \
                    --workflow_id "$WORKFLOW_ID" \
                    --pipeline_name "${{ inputs.pipeline_name }}")
                    ALL_OUTPUTS+="$WORKFLOW_OUTPUT"$'\n'
              done
              break
            done
          done
          
          # Generate final markdown and JSON summary
          FINAL_SUMMARY="## Combined Workflow Statuses\n\n"
          JSON_SUMMARY='{"summary":['
          FIRST_ENTRY=true

           # Add all workflow statuses to the summary
          for SUBMISSION_ID in "${!WORKFLOW_STATUSES[@]}"; do
            # Generate the submission URL and append to the summary
            SUBMISSION_URL="https://app.terra.bio/#workspaces/$WORKSPACE_NAMESPACE/WARP%20Tests/job_history/$SUBMISSION_ID"
            # Get the input file name from the mapping
            INPUT_FILE_NAME="${SUBMISSION_ID_TO_FILENAME[$SUBMISSION_ID]}"
            # Append the submission details to the final summary
            FINAL_SUMMARY+="[${INPUT_FILE_NAME}]($SUBMISSION_URL)\n"
            # Append the workflow statuses for this submission
            FINAL_SUMMARY+="${WORKFLOW_STATUSES[$SUBMISSION_ID]}\n\n"

            # Parse workflow IDs and statuses for JSON
            while IFS='|' read -r WORKFLOW_ID STATUS; do
              # Trim whitespace
              WORKFLOW_ID=$(echo "$WORKFLOW_ID" | xargs)
              STATUS=$(echo "$STATUS" | xargs)

              # Skip header row
              if [ "$WORKFLOW_ID" != "Workflow ID" ] && [ "$WORKFLOW_ID" != "---" ]; then
                # Add comma separator after first entry
                if [ "$FIRST_ENTRY" = false ]; then
                  JSON_SUMMARY+=','
                else
                  FIRST_ENTRY=false
                fi

                # Add JSON entry
                JSON_SUMMARY+="{\"id\":\"$WORKFLOW_ID\",\"filename\":\"$INPUT_FILE_NAME\",\"url\":\"$SUBMISSION_URL\",\"status\":\"$STATUS\"}"
              fi
            done <<< "${WORKFLOW_STATUSES[$SUBMISSION_ID]}"
          done
          
          # Close JSON array and object
          JSON_SUMMARY+=']}'

          echo "$JSON_SUMMARY" | jq '.' | tee workflow_summary.json
          echo -e "$FINAL_SUMMARY" >> $GITHUB_STEP_SUMMARY
          
          if [ "$OVERALL_SUCCESS" = false ]; then
              echo ""
              echo ""
              echo "****************************************************************************************"
              echo "****************************************************************************************"
              echo ""
              echo "One or more workflows failed in Terra. Check the workflow status summary for details."
              echo ""
              echo "****************************************************************************************"
              echo "****************************************************************************************"
              echo ""
              echo ""
              exit 1
          fi

      # Step 12: Cleanup
      # Purpose: Ensures cleanup of Terra method configurations regardless of test outcome
      - name: Delete Method Configuration
        if: always()
        run: |
          echo "Deleting method configuration for branch: $BRANCH_NAME"
          DELETE_RESPONSE=$(python3 scripts/firecloud_api/firecloud_api.py delete_method_config \
          --workspace-namespace $WORKSPACE_NAMESPACE \
          --workspace-name "$TESTING_WORKSPACE" \
          --pipeline_name "${{ inputs.pipeline_name }}" \
          --branch_name "$BRANCH_NAME" \
          --test_type "$testType" \
          --sa-json-b64 "$SA_JSON_B64" \
          --user "$USER" \
          --method_config_name "$METHOD_CONFIG_NAME")
          
          echo "Delete response: $DELETE_RESPONSE"
          if [ "$DELETE_RESPONSE" == "True" ]; then
            echo "Method configuration deleted successfully."
          else
            echo "Error: Method configuration deletion failed."
            exit 1
          fi

      # Step 13: Upload Summary Artifact
      # Purpose: Uploads the workflow summary JSON as an artifact for further analysis
      - name: Upload Summary Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-summary
          path: workflow_summary.json

      # Step 14: Post Summary Artifact
      # Purpose: Sends the workflow summary JSON to the workflow analysis processor
      - name: Post Summary Artifact
        if: always()
        env:
          WORKFLOW_RUN_ID: ${{ github.run_id }}
        run: |
          if [ -f "workflow_summary.json" ]; then
            jq -n --arg RUN_ID "$WORKFLOW_RUN_ID" --slurpfile SUMMARY workflow_summary.json '{run_id: $RUN_ID, summary: $SUMMARY[0].summary}' > workflow_payload.json
            curl -X POST "https://workflow-processor-5zhxczqoxq-uc.a.run.app/api/workflow-summary/" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer ${{ github.token }}" \
              -d @workflow_payload.json
          else
            echo "Warning: workflow_summary.json not found. Check for earlier failures."
          fi

      # Step 15: Print Summary on Success
      # Purpose: Prints the final summary of the pipeline execution in case of success
      - name: Print Summary on Success
        if: success()
        run: |
          echo "# :white_check_mark: Pipeline Execution Summary :white_check_mark:" >> $GITHUB_STEP_SUMMARY

      # Step 16: Print Summary on Failure
      # Purpose: Prints the final summary of the pipeline execution in case of failure
      - name: Print Summary on Failure
        if: failure()
        run: |
          echo "# :x: Pipeline Execution Summary (on Failure) :x: " >> $GITHUB_STEP_SUMMARY
